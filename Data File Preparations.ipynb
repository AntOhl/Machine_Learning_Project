{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Doc\n",
    "import datetime\n",
    "import dateparser\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = [20,10]\n",
    "\n",
    "#Set sentiment extensions\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_scores(docx):\n",
    "    return sent_analyzer.polarity_scores(docx.text)\n",
    "Doc.set_extension(\"sentimenter\",getter=sentiment_scores,force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trump became the president as of January 20, 2017, but here we extract all the tweets as of 2007.\n",
    "#Also we need to remove retweets\n",
    "df1 = pd.read_excel('tweets.xls', parse_dates=['created_at'])\n",
    "df1 = df1[df1['created_at'] > dateparser.parse('01/01/2017 UTC')]\n",
    "df1 = df1[df1['is_retweet'] == False]\n",
    "df1 = df1.drop(['id_str', 'is_retweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove trivial tweets\n",
    "#For now I just removed all the tweets with a length of smaller than 5.\n",
    "df1['len'] = df1['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df1 = df1[df1['len']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the tweets by day\n",
    "df2 = df1.groupby(df1['created_at'].dt.date)['text'].apply(lambda x:' '.join(x))\n",
    "df2 = pd.DataFrame(df2)\n",
    "df2.index = pd.to_datetime(df2.index, format='%Y-%m-%d')\n",
    "df2['num'] = df1.groupby(df1['created_at'].dt.date)['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the stock price file\n",
    "Y_day = pd.read_excel('S&P 500.xls')\n",
    "Y_day['is_up'] = (Y_day['Close'].shift(-1) > Y_day['Close']) * 1\n",
    "Y_day['diff'] = Y_day['Close'].shift(-1) - Y_day['Close']\n",
    "Y_day = Y_day[['Date', 'is_up', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join both dataframe together\n",
    "df = df2.merge(Y_day, left_on='created_at', right_on='Date')\n",
    "df.columns = ['text', 'num', 'date', 'is_up', 'diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe\n",
    "df_temp = df\n",
    "df_temp['date'] = df_temp['date'].astype(str)\n",
    "df_temp.to_excel('tweets_new.xls', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply NLP models on the remaining tweets\n",
    "def nlp_apply(file_from, file_to):\n",
    "    df = pd.read_excel(file_from)\n",
    "    tweets = df['text']\n",
    "\n",
    "    doc_bin = DocBin(attrs=['LEMMA', 'ENT_IOB', 'ENT_TYPE', 'is_digit', 'like_url', 'like_num', \n",
    "                            'DEP', \"POS\", 'TAG', 'SHAPE', 'is_alpha', 'is_stop', 'is_oov'], store_user_data=True)\n",
    "\n",
    "    for doc in nlp.pipe(tweets):\n",
    "        doc_bin.add(doc)\n",
    "    bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "    #Save to a pickle file\n",
    "    pickle.dump(bytes_data, open(file_to, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A help function to read the pickle file\n",
    "def read_data(path):\n",
    "    tweets = pickle.load(open(path, 'rb'))\n",
    "    doc_bin = DocBin().from_bytes(tweets)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "    return docs\n",
    "tweets = read_data('tweets_new.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into a training and holdout set(it is for the final validation, not for the CV!)\n",
    "#The holdout set begins as of September of 2019\n",
    "\n",
    "df_train = df.iloc[:657]\n",
    "df_test = df.iloc[657:]\n",
    "df_train.to_excel('tweets_train.xls', index=False)\n",
    "df_test.to_excel('tweets_test.xls', index=False)\n",
    "nlp_apply('tweets_train.xls', 'tweets_train.p')\n",
    "nlp_apply('tweets_test.xls', 'tweets_test.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment for the hourly data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trump became the president as of January 20, 2017, but here we extract all the tweets as of 2007.\n",
    "#Also we need to remove retweets\n",
    "df1 = pd.read_excel('tweets.xls', parse_dates=['created_at'])\n",
    "df1 = df1[df1['created_at'] > dateparser.parse('01/01/2017 UTC')]\n",
    "df1 = df1[df1['is_retweet'] == False]\n",
    "df1 = df1.drop(['id_str', 'is_retweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove trivial tweets\n",
    "#For now I just removed all the tweets with a length of smaller than 5.\n",
    "df1['len'] = df1['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df1 = df1[df1['len']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the tweets by hour\n",
    "df1 = df1.set_index(['created_at'])\n",
    "df1 = df1.resample('60Min')['text'].apply(lambda x:' '.join(x))\n",
    "df2 = pd.DataFrame(df1)\n",
    "df2 = df2[df2['text']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#Process the stock price file\n",
    "Y_hour = yf.download(\"^GSPC\", start=\"2018-01-01\", end=\"2019-11-25\", interval='60m')\n",
    "Y_hour.index = Y_hour.index.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join both dataframe together\n",
    "df2['get_close_price'] = df2.index + pd.Timedelta(minutes=30)\n",
    "df2['get_open_price'] = df2.index - pd.Timedelta(minutes=30)\n",
    "df2 = df2.reset_index()\n",
    "df2 = df2.merge(Y_hour['Open'], left_on='get_open_price', right_on=Y_hour.index, how='left')\n",
    "df2 = df2.merge(Y_hour['Close'], left_on='get_close_price', right_on=Y_hour.index, how='left')\n",
    "df2 = df2.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns without price data\n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Make labels\n",
    "df2['is_up'] = (df2['Close'] > df2['Open']) * 1\n",
    "df2['diff'] = df2['Close'] - df2['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into a training and holdout set(it is for the final validation, not for the CV!)\n",
    "#The holdout set begins as of September of 2019\n",
    "df = df2.tz_localize(None)\n",
    "df['get_close_price'] = df['get_close_price'].astype(str)\n",
    "df['get_open_price'] = df['get_open_price'].astype(str)\n",
    "\n",
    "df_train = df.iloc[:638]\n",
    "df_test = df.iloc[638:]\n",
    "df_train.to_excel('tweets_train_hourly.xls', index=False)\n",
    "df_test.to_excel('tweets_test_hourly.xls', index=False)\n",
    "nlp_apply('tweets_train_hourly.xls', 'tweets_train_hourly.p')\n",
    "nlp_apply('tweets_test_hourly.xls', 'tweets_test_hourly.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
