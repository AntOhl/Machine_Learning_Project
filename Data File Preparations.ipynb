{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Doc\n",
    "import datetime\n",
    "import dateparser\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = [20,10]\n",
    "\n",
    "#Set sentiment extensions\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_scores(docx):\n",
    "    return sent_analyzer.polarity_scores(docx.text)\n",
    "Doc.set_extension(\"sentimenter\",getter=sentiment_scores,force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Daily Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trump became the president as of January 20, 2017, but here we extract all the tweets as of 2007.\n",
    "#Also we need to remove retweets\n",
    "df1 = pd.read_excel('tweets.xls', parse_dates=['created_at'])\n",
    "df1 = df1[df1['created_at'] > dateparser.parse('01/01/2017 UTC')]\n",
    "df1 = df1[df1['is_retweet'] == False]\n",
    "df1 = df1.drop(['id_str', 'is_retweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change timezone to the NYC timezone\n",
    "df1 = df1.set_index('created_at')\n",
    "df1.index = df1.index.tz_convert('EST')\n",
    "df1 = df1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove trivial tweets\n",
    "#For now I just removed all the tweets with a length of smaller than 5.\n",
    "df1['len'] = df1['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df1 = df1[df1['len']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the tweets by day\n",
    "df2 = df1.groupby(df1['created_at'].dt.date)['text'].apply(lambda x:' '.join(x))\n",
    "df2 = pd.DataFrame(df2)\n",
    "df2.index = pd.to_datetime(df2.index, format='%Y-%m-%d')\n",
    "df2['num'] = df1.groupby(df1['created_at'].dt.date)['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the stock price file\n",
    "Y_day = pd.read_excel('S&P 500.xls')\n",
    "Y_day['is_up'] = (Y_day['Close'].shift(-1) > Y_day['Close']) * 1\n",
    "Y_day['diff'] = Y_day['Close'].shift(-1) - Y_day['Close']\n",
    "Y_day = Y_day[['Date', 'is_up', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join both dataframe together\n",
    "df = df2.merge(Y_day, left_on='created_at', right_on='Date')\n",
    "df.columns = ['text', 'num', 'date', 'is_up', 'diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe\n",
    "df_temp = df\n",
    "df_temp['date'] = df_temp['date'].astype(str)\n",
    "df_temp.to_excel('tweets_new.xls', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply NLP models on the remaining tweets\n",
    "def nlp_apply(file_from, file_to):\n",
    "    df = pd.read_excel(file_from)\n",
    "    tweets = df['text']\n",
    "\n",
    "    doc_bin = DocBin(attrs=['LEMMA', 'ENT_IOB', 'ENT_TYPE', 'is_digit', 'like_url', 'like_num', \n",
    "                            'DEP', \"POS\", 'TAG', 'SHAPE', 'is_alpha', 'is_stop', 'is_oov'], store_user_data=True)\n",
    "\n",
    "    for doc in nlp.pipe(tweets):\n",
    "        doc_bin.add(doc)\n",
    "    bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "    #Save to a pickle file\n",
    "    pickle.dump(bytes_data, open(file_to, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A help function to read the pickle file\n",
    "def read_data(path):\n",
    "    tweets = pickle.load(open(path, 'rb'))\n",
    "    doc_bin = DocBin().from_bytes(tweets)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into a training and holdout set(it is for the final validation, not for the CV!)\n",
    "#The holdout set begins as of September of 2019\n",
    "\n",
    "df_train = df.iloc[:654]\n",
    "df_test = df.iloc[654:]\n",
    "df_train.to_excel('tweets_train_daily.xls', index=False)\n",
    "df_test.to_excel('tweets_test_daily.xls', index=False)\n",
    "nlp_apply('tweets_train_daily.xls', 'tweets_train_daily.p')\n",
    "nlp_apply('tweets_test_daily.xls', 'tweets_test_daily.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Hourly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trump became the president as of January 20, 2017, but here we extract all the tweets as of 2007.\n",
    "#Also we need to remove retweets\n",
    "df1 = pd.read_excel('tweets.xls', parse_dates=['created_at'])\n",
    "df1 = df1[df1['created_at'] > dateparser.parse('01/01/2017 UTC')]\n",
    "df1 = df1[df1['is_retweet'] == False]\n",
    "df1 = df1.drop(['id_str', 'is_retweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove trivial tweets\n",
    "#For now I just removed all the tweets with a length of smaller than 5.\n",
    "df1['len'] = df1['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df1 = df1[df1['len']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the tweets by hour\n",
    "df1 = df1.set_index(['created_at'])\n",
    "df2 = df1.resample('60Min')['text'].apply(lambda x:' '.join(x))\n",
    "df2 = pd.DataFrame(df2)\n",
    "df2['num'] = df1.resample('60Min')['text'].count()\n",
    "df2 = df2[df2['num']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#Process the stock price file\n",
    "Y_hour = yf.download(\"^GSPC\", start=\"2018-01-01\", end=\"2019-11-25\", interval='60m')\n",
    "Y_hour.index = Y_hour.index.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join both dataframe together\n",
    "df2['get_close_price'] = df2.index + pd.Timedelta(minutes=30)\n",
    "df2['get_open_price'] = df2.index - pd.Timedelta(minutes=30)\n",
    "df2 = df2.reset_index()\n",
    "df2 = df2.merge(Y_hour['Open'], left_on='get_open_price', right_on=Y_hour.index, how='left')\n",
    "df2 = df2.merge(Y_hour['Close'], left_on='get_close_price', right_on=Y_hour.index, how='left')\n",
    "df2 = df2.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NAs to reduce the waste of the dataset, but it didn't seem to work\n",
    "#df2['Close'] = df2['Close'].fillna(method='bfill')\n",
    "#df2['Open'] = df2['Open'].fillna(method='ffill')\n",
    "\n",
    "#Drop columns without price data\n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make labels\n",
    "df2['is_up'] = (df2['Close'] > df2['Open']) * 1\n",
    "df2['diff'] = df2['Close'] - df2['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into a training and holdout set(it is for the final validation, not for the CV!)\n",
    "#The holdout set begins as of September of 2019\n",
    "df = df2.tz_localize(None)\n",
    "df['get_close_price'] = df['get_close_price'].astype(str)\n",
    "df['get_open_price'] = df['get_open_price'].astype(str)\n",
    "\n",
    "df_train = df.iloc[:686]\n",
    "df_test = df.iloc[686:]\n",
    "df_train.to_excel('tweets_train_hourly.xls', index=False)\n",
    "df_test.to_excel('tweets_test_hourly.xls', index=False)\n",
    "nlp_apply('tweets_train_hourly.xls', 'tweets_train_hourly.p')\n",
    "nlp_apply('tweets_test_hourly.xls', 'tweets_test_hourly.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
